{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, count, when, col, desc, udf, col, sort_array, asc, avg, explode,array,lit,coalesce,countDistinct\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer,VectorAssembler\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.types import IntegerType\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession \\\n",
    "        .builder\\\n",
    "        .appName('Sparkify')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Exploratory Data\n",
    "The mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_clean:\n",
    "    \n",
    "    \"\"\"\n",
    "    Build a class to be load and show the characters of dataset and remove the records without \"userId\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        \"\"\"\n",
    "        INPUT: (json) dataset file\n",
    "    \n",
    "        OUTPUT: print the characters of dataset:The amount of record, the Schema, the summary of missing value\n",
    "        \"\"\"\n",
    "        #read dataset from file\n",
    "        self.data = dataset\n",
    "        self.user_log = spark.read.json(self.data)\n",
    "        \n",
    "        #print the characters of dataset\n",
    "        print('the data has been load...')\n",
    "        print('----------------------------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "        print('The amount of record in origin dataset:', self.user_log.count(),'.The amount of fields in origin dataset:',len(self.user_log.columns))\n",
    "        print('----------------------------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "        print('the first record is', self.user_log.head())\n",
    "        print('----------------------------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "        print('the Schema:')\n",
    "        print(self.user_log.printSchema())\n",
    "        print('----------------------------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "        print('the summary of missing value')\n",
    "        print(self.user_log.select([count(when(col(colu).isNull() | isnan(colu) , colu))\\\n",
    "         .alias(colu) for colu in self.user_log.columns])\\\n",
    "         .collect())\n",
    "        print('----------------------------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "    def clean(self):\n",
    "        \n",
    "        # remove rows with empty userId\n",
    "        self.user_log_cleaned = self.user_log.filter(self.user_log[\"userId\"] != \"\")\n",
    "        print('The cleaned data has been created')\n",
    "        print('----------------------------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "        #show the amount of records in new user_log\n",
    "        print('The amount of records in The cleaned data:' , self.user_log_cleaned.count())\n",
    "        print('----------------------------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "        return  self.user_log_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has been load...\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "The amount of record in origin dataset: 286500 .The amount of fields in origin dataset: 18\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "the first record is Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "the Schema:\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "None\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "the summary of missing value\n",
      "[Row(artist=58392, auth=0, firstName=8346, gender=8346, itemInSession=0, lastName=8346, length=58392, level=0, location=8346, method=0, page=0, registration=8346, sessionId=0, song=58392, status=0, ts=0, userAgent=8346, userId=0)]\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "The cleaned data has been created\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "The amount of records in The cleaned data: 278154\n",
      "----------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset = 'mini_sparkify_event_data.json'\n",
    "\n",
    "#Get data\n",
    "data = load_clean(dataset).clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnsGroupbyType(input_dataframe):\n",
    "                       \n",
    "    \"\"\"\n",
    "    INPUT: dataset\n",
    "    OUTPUT: A series include the groups of columns \n",
    "    \"\"\"\n",
    "    type_df = pd.DataFrame(input_dataframe.dtypes)\n",
    "    type_df.columns = ['column', 'type']\n",
    "    result = (type_df.groupby('type')['column'].apply(list), type_df.groupby('type')['column'].apply(list).index[:])\n",
    "    \n",
    "    print('There are {} typies values in dataset.'.format(len(result[1])))\n",
    "    print('Show below...')\n",
    "    for i in range(len(result[1])):\n",
    "        print('typy: ',result[1][i])\n",
    "        print(result[0][i])\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_describe(info, dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    INPUT: dataset, information of columns\n",
    "    OUTPUT: the static analysis of columns\n",
    "    \"\"\"\n",
    "    for i in range(len(info[1])):\n",
    "        if len(info[0][i])<=8:\n",
    "            dataset.select(info[0][i]) \\\n",
    "                 .describe() \\\n",
    "                 .show()\n",
    "        else:\n",
    "            start  = 0\n",
    "            end = 8\n",
    "            while end < len(info[0][i]):\n",
    "                dataset.select(info[0][i][start : end]) \\\n",
    "                 .describe() \\\n",
    "                 .show()    \n",
    "                start = end\n",
    "                end = end + 8\n",
    "            if start <  len(info[0][i]):\n",
    "                 dataset.select(info[0][i][start:]) \\\n",
    "                 .describe() \\\n",
    "                 .show()  \n",
    "                          \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 typies values in dataset.\n",
      "Show below...\n",
      "typy:  bigint\n",
      "['itemInSession', 'registration', 'sessionId', 'status', 'ts']\n",
      "typy:  double\n",
      "['length']\n",
      "typy:  string\n",
      "['artist', 'auth', 'firstName', 'gender', 'lastName', 'level', 'location', 'method', 'page', 'song', 'userAgent', 'userId']\n",
      "+-------+------------------+--------------------+------------------+------------------+--------------------+\n",
      "|summary|     itemInSession|        registration|         sessionId|            status|                  ts|\n",
      "+-------+------------------+--------------------+------------------+------------------+--------------------+\n",
      "|  count|            278154|              278154|            278154|            278154|              278154|\n",
      "|   mean|114.89918174824018|1.535358834084427...|1042.5616241362698|209.10321620397335|1.540958915431871...|\n",
      "| stddev|  129.851729399489| 3.291321616327586E9| 726.5010362219813|30.151388851328214|1.5068287123306298E9|\n",
      "|    min|                 0|       1521380675000|                 1|               200|       1538352117000|\n",
      "|    max|              1321|       1543247354000|              2474|               404|       1543799476000|\n",
      "+-------+------------------+--------------------+------------------+------------------+--------------------+\n",
      "\n",
      "+-------+-----------------+\n",
      "|summary|           length|\n",
      "+-------+-----------------+\n",
      "|  count|           228108|\n",
      "|   mean|249.1171819778458|\n",
      "| stddev|99.23517921058361|\n",
      "|    min|          0.78322|\n",
      "|    max|       3024.66567|\n",
      "+-------+-----------------+\n",
      "\n",
      "+-------+------------------+---------+---------+------+--------+------+-----------------+------+\n",
      "|summary|            artist|     auth|firstName|gender|lastName| level|         location|method|\n",
      "+-------+------------------+---------+---------+------+--------+------+-----------------+------+\n",
      "|  count|            228108|   278154|   278154|278154|  278154|278154|           278154|278154|\n",
      "|   mean| 551.0852017937219|     null|     null|  null|    null|  null|             null|  null|\n",
      "| stddev|1217.7693079161374|     null|     null|  null|    null|  null|             null|  null|\n",
      "|    min|               !!!|Cancelled| Adelaida|     F|   Adams|  free|       Albany, OR|   GET|\n",
      "|    max| ÃÂlafur Arnalds|Logged In|   Zyonna|     M|  Wright|  paid|Winston-Salem, NC|   PUT|\n",
      "+-------+------------------+---------+---------+------+--------+------+-----------------+------+\n",
      "\n",
      "+-------+-------+--------------------+--------------------+-----------------+\n",
      "|summary|   page|                song|           userAgent|           userId|\n",
      "+-------+-------+--------------------+--------------------+-----------------+\n",
      "|  count| 278154|              228108|              278154|           278154|\n",
      "|   mean|   null|            Infinity|                null|59682.02278593872|\n",
      "| stddev|   null|                 NaN|                null|109091.9499991047|\n",
      "|    min|  About|\u001c",
      "ÃÂg ÃÂtti Gr...|\"Mozilla/5.0 (Mac...|               10|\n",
      "|    max|Upgrade|ÃÂau hafa slopp...|Mozilla/5.0 (comp...|               99|\n",
      "+-------+-------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_columns = columnsGroupbyType(data)\n",
    "data_describe(info_columns,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT userId)|\n",
      "+----------------------+\n",
      "|                   225|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the numders of user\n",
    "number_users = data.select(countDistinct(\"userId\"))\n",
    "number_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsitribution_column(col, dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT: dataset, specfic columns\n",
    "    OUTPUT: the distribution of specific columns in dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    for c in col:\n",
    "\n",
    "        col_stat = dataset.groupBy(c).count().orderBy(c)\n",
    "        col_stat.show()\n",
    "        ax = sns.barplot(x=c,y='count',data=col_stat.toPandas(),color='blue')\n",
    "        plt.savefig('{}.jpg'.format(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|gender| count|\n",
      "+------+------+\n",
      "|     F|154578|\n",
      "|     M|123576|\n",
      "+------+------+\n",
      "\n",
      "+-----+------+\n",
      "|level| count|\n",
      "+-----+------+\n",
      "| free| 55721|\n",
      "| paid|222433|\n",
      "+-----+------+\n",
      "\n",
      "+------+------+\n",
      "|method| count|\n",
      "+------+------+\n",
      "|   GET| 20336|\n",
      "|   PUT|257818|\n",
      "+------+------+\n",
      "\n",
      "+------+------+\n",
      "|status| count|\n",
      "+------+------+\n",
      "|   200|254718|\n",
      "|   307| 23184|\n",
      "|   404|   252|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee89450908>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = [\"gender\",\"level\",\"method\",\"status\"]\n",
    "dsitribution_column(columns, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create churn label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                page|page_count|\n",
      "+--------------------+----------+\n",
      "|              Cancel|        52|\n",
      "|    Submit Downgrade|        63|\n",
      "|         Thumbs Down|      2546|\n",
      "|                Home|     10082|\n",
      "|           Downgrade|      2055|\n",
      "|         Roll Advert|      3933|\n",
      "|              Logout|      3226|\n",
      "|       Save Settings|       310|\n",
      "|Cancellation Conf...|        52|\n",
      "|               About|       495|\n",
      "|            Settings|      1514|\n",
      "|     Add to Playlist|      6526|\n",
      "|          Add Friend|      4277|\n",
      "|            NextSong|    228108|\n",
      "|           Thumbs Up|     12551|\n",
      "|                Help|      1454|\n",
      "|             Upgrade|       499|\n",
      "|               Error|       252|\n",
      "|      Submit Upgrade|       159|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#page values\n",
    "page_stat= data.groupby('page').agg({'page':'count'}).withColumnRenamed(\"count(page)\", \"page_count\")\n",
    "page_stat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userID|\n",
      "+------+\n",
      "|18    |\n",
      "|18    |\n",
      "|32    |\n",
      "|32    |\n",
      "|125   |\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cancel_Cancellation_account = data.filter(col('page').isin(['Cancel','Cancellation Confirmation'])).select(['userID'])\n",
    "cancel_Cancellation_account.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of cancel is same with the number of cancellation conformation. users get cancel conformation after did cancel action. This result show that the 'cancel' and 'Cancellation Confirmation' are duplicate records. Then I would only select 'Cancellation Confirmation' as churn = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', label=0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define churn\n",
    "define_churn = udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())\n",
    "# add feature label \n",
    "data_new = data.withColumn(\"label\", define_churn(\"page\"))\n",
    "# print top 5\n",
    "data_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def creat_features(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    create new features\n",
    "    INPUT: dataset\n",
    "    OUTPUT: dataset with new features\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = [\"userId\",\"song\", \"gender\", \"level\", \"page\", \"label\"]\n",
    "    data = data_new.select(columns)\n",
    "    # define gender_new\n",
    "    define_gender_new = udf(lambda x: 1 if x == \"M\" else 0, IntegerType())\n",
    "    # add feature GenderNew\n",
    "    data = data.withColumn(\"GenderNew\", define_gender_new(\"gender\"))\n",
    "\n",
    "\n",
    "    # define level_new\n",
    "    define_level_new = udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())\n",
    "    # add feature LevelNew\n",
    "    data = data.withColumn(\"LevelNew\", define_level_new(\"level\"))\n",
    "\n",
    "    # define downgrade\n",
    "    define_downgrade = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())\n",
    "    # add feature downgrade\n",
    "    data = data.withColumn(\"Downgrade\", define_downgrade(\"page\"))\n",
    "\n",
    "    # define thumbs_up\n",
    "    define_thumbs_up = udf(lambda x: 1 if x == \"Thumbs Up\" else 0, IntegerType())\n",
    "    # add thumbs variables\n",
    "    data = data.withColumn(\"ThumbsUp\", define_thumbs_up(\"page\"))\n",
    "\n",
    "    # define thumbs_down\n",
    "    define_thumbs_down = udf(lambda x: 1 if x == \"Thumbs Down\" else 0, IntegerType())\n",
    "    # add thumbs variables\n",
    "    data= data.withColumn(\"ThumbsDown\", define_thumbs_down(\"page\"))\n",
    "\n",
    "    # define NextSong\n",
    "    define_NextSong = udf(lambda x: 1 if x == \"NextSong\" else 0, IntegerType())\n",
    "    # add songs variable\n",
    "    data = data.withColumn(\"NextSong\", define_NextSong(\"page\"))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+--------+----------+---------+--------+\n",
      "|userId|GenderNew|LevelNew|ThumbsUp|ThumbsDown|Downgrade|NextSong|\n",
      "+------+---------+--------+--------+----------+---------+--------+\n",
      "|    30|        1|       1|       0|         0|        0|       1|\n",
      "|     9|        1|       0|       0|         0|        0|       1|\n",
      "|    30|        1|       1|       0|         0|        0|       1|\n",
      "|     9|        1|       0|       0|         0|        0|       1|\n",
      "|    30|        1|       1|       0|         0|        0|       1|\n",
      "+------+---------+--------+--------+----------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_new = creat_features(data_new)\n",
    "#show the new features\n",
    "data_new.select([\"userId\",\"GenderNew\",\"LevelNew\",\"ThumbsUp\", \"ThumbsDown\",\"Downgrade\",\"NextSong\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+--------+------------+------------+--------------+-----+\n",
      "|userId|Downgrade|GenderNew|LevelNew|sum_NextSong|sum_thumbsUp|sum_ThumbsDown|label|\n",
      "+------+---------+---------+--------+------------+------------+--------------+-----+\n",
      "|100010|        0|        0|       0|         275|          17|             5|    0|\n",
      "|200002|        0|        1|       1|         387|          21|             6|    0|\n",
      "|   125|        0|        1|       0|           8|           0|             0|    1|\n",
      "|    51|        0|        1|       1|        2111|         100|            21|    1|\n",
      "|   124|        0|        0|       1|        4079|         171|            41|    0|\n",
      "+------+---------+---------+--------+------------+------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def new_features(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    data engineering for some columns, example: the sum of nextsong\n",
    "    INPUT: dataset\n",
    "    OUTPUT: \n",
    "    \"\"\"\n",
    "    \n",
    "    data_new_features = data.groupby(data_new.userId).agg({\"Downgrade\": \"max\", \n",
    "                                                  \"GenderNew\": \"max\",\n",
    "                                                  \"LevelNew\": \"max\",\n",
    "                                                  \"NextSong\": \"sum\",\n",
    "                                                  \"ThumbsUp\": \"sum\",\n",
    "                                                  \"ThumbsDown\": \"sum\",\n",
    "                                                  \"label\": \"max\"}) \n",
    "    data_new_features = data_new_features.select(col(\"userId\"),\n",
    "                                       col(\"max(Downgrade)\").alias(\"Downgrade\"),\n",
    "                                       col(\"max(GenderNew)\").alias(\"GenderNew\"),\n",
    "                                       col(\"max(LevelNew)\").alias(\"LevelNew\"),\n",
    "                                       col(\"sum(NextSong)\").alias(\"sum_NextSong\"),\n",
    "                                       col(\"sum(ThumbsUp)\").alias(\"sum_thumbsUp\"),\n",
    "                                       col(\"sum(ThumbsDown)\").alias(\"sum_ThumbsDown\"),\n",
    "                                       col(\"max(label)\").alias(\"label\"))\n",
    "    return data_new_features\n",
    "\n",
    "# present result\n",
    "data_new_features = new_features(data_new)\n",
    "data_new_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling(data):\n",
    "    a= data.filter(col(\"label\") == False).count()\n",
    "    b = data.filter(col(\"label\") == True).count()\n",
    "    if a > b:\n",
    "        major_user_log = data.filter(col(\"label\") == False)\n",
    "        minor_user_log = data.filter(col(\"label\") == True)\n",
    "    else:\n",
    "        major_user_log = data.filter(col(\"label\") == True)\n",
    "        minor_user_log = data.filter(col(\"label\") == False)\n",
    "        \n",
    "    ratio = int(major_user_log.count()/minor_user_log.count())\n",
    "    print(\"the amount of majority : {}.   the amount of minority : {} \".format(major_user_log.count(),minor_user_log.count()))\n",
    "    print(\"the ratio is {}\".format(ratio))\n",
    "\n",
    "    # duplicate the minority rows\n",
    "    oversampled_user_log = minor_user_log.withColumn(\"dummy\", explode(array([lit(x) for x in range(ratio)]))).drop('dummy')\n",
    "    # combine both oversampled minority rows and previous majority rows \n",
    "    combined_data = major_user_log.unionAll(oversampled_user_log)\n",
    "    print(\"The amount of records is {} after oversampling\".format(combined_data.count()))\n",
    "\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the amount of majority : 173.   the amount of minority : 52 \n",
      "the ratio is 3\n",
      "The amount of records is 329 after oversampling\n"
     ]
    }
   ],
   "source": [
    "combined_df = oversampling(data_new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About unbalence problem\n",
    "\n",
    "class (label) 1 has 52 recoeds, while class 0 has only 173 recoeds. We can undersample class 0, or oversample class 1.\n",
    "These changes are called sampling the dataset \n",
    "Here I choosed the oversampling, It is to duplicate the samples from under-represented class, to inflate the numbers till it reaches the same level as the dominant class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_feature(data):\n",
    "    #transfer the features to vector\n",
    "    assembler = VectorAssembler(inputCols=[\"Downgrade\", \"LevelNew\", \"sum_NextSong\", \"sum_thumbsUp\",\"sum_ThumbsDown\"], outputCol=\"features\")\n",
    "    combined_data = assembler.transform(data)\n",
    "    combined_data.select('label', 'features').show(4)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[0.0,0.0,275.0,17...|\n",
      "|    0|[0.0,1.0,387.0,21...|\n",
      "|    0|[0.0,1.0,4079.0,1...|\n",
      "|    0|[0.0,0.0,150.0,7....|\n",
      "+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df = transfer_feature(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#split data in train dataset, validation dataset and test dataset.\n",
    "(trainData, tempData)= combined_df.randomSplit(weights = [0.70,0.30])\n",
    "(valiData, testData)= tempData.randomSplit(weights = [0.50,0.50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(prediction, data_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    INPUT: prediction model, data_name \n",
    "    OUTPUT: the F1 score, accuracy, error of model\n",
    "    \"\"\"\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(prediction)\n",
    "    f1_score_evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\",metricName='f1')\n",
    "    f1_score = f1_score_evaluator.evaluate(prediction)\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"Evaluation on \", data_name)\n",
    "    print(\"Accuracy = %g\" % accuracy)\n",
    "    print(\"Validation Error = %g\" % (1.0 - accuracy))\n",
    "    print(\"F1 score = %g\" % (f1_score))\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, label_new])\n",
    "param_grid = ParamGridBuilder().\\\n",
    "            addGrid(rf.maxDepth, [4,5,6]).\\\n",
    "            addGrid(rf.numTrees, [5,7]).\\\n",
    "            build()\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps = param_grid, \n",
    "                    evaluator = MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                    numFolds=3)\n",
    "cv_model = cv.fit(trainData)\n",
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StringIndexer_999be3dcf697, VectorIndexer_249c585e42a0, RandomForestClassificationModel (uid=RandomForestClassifier_9abc2d682489) with 5 trees, IndexToString_00f2cef00853]\n"
     ]
    }
   ],
   "source": [
    "print(cv_model.bestModel.stages)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: RandomForestClassifier_9abc2d682489\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "Evaluation on  validation data\n",
      "Accuracy = 0.77551\n",
      "Validation Error = 0.22449\n",
      "F1 score = 0.774381\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "Evaluation on  test data\n",
      "Accuracy = 0.733333\n",
      "Validation Error = 0.266667\n",
      "F1 score = 0.733333\n",
      "-----------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('model:',rf)\n",
    "\n",
    "#run function evaluation on validation dataset\n",
    "predictions = best_model.transform(valiData)\n",
    "evaluation(predictions, 'validation data')\n",
    "\n",
    "#run function evaluation on test dataset\n",
    "predictions = best_model.transform(testData)\n",
    "evaluation(predictions,'test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Inprovement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has been load...\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "The amount of record in origin dataset: 286500 .The amount of fields in origin dataset: 18\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "the first record is Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "the Schema:\n",
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "None\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "the summary of missing value\n",
      "[Row(artist=58392, auth=0, firstName=8346, gender=8346, itemInSession=0, lastName=8346, length=58392, level=0, location=8346, method=0, page=0, registration=8346, sessionId=0, song=58392, status=0, ts=0, userAgent=8346, userId=0)]\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "The cleaned data has been created\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "The amount of records in The cleaned data: 278154\n",
      "----------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30', label=0)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = 'mini_sparkify_event_data.json'\n",
    "#Get data\n",
    "data = load_clean(dataset).clean()\n",
    "# define churn\n",
    "define_churn = udf(lambda x: 1 if x == \"Cancellation Confirmation\" or x ==  \"Downgrade\" else 0, IntegerType())\n",
    "# add feature label \n",
    "data_new = data.withColumn(\"label\", define_churn(\"page\"))\n",
    "# print top 5\n",
    "data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+--------+----------+---------+--------+\n",
      "|userId|GenderNew|LevelNew|ThumbsUp|ThumbsDown|Downgrade|NextSong|\n",
      "+------+---------+--------+--------+----------+---------+--------+\n",
      "|    30|        1|       1|       0|         0|        0|       1|\n",
      "|     9|        1|       0|       0|         0|        0|       1|\n",
      "|    30|        1|       1|       0|         0|        0|       1|\n",
      "|     9|        1|       0|       0|         0|        0|       1|\n",
      "|    30|        1|       1|       0|         0|        0|       1|\n",
      "+------+---------+--------+--------+----------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+---------+---------+--------+------------+------------+--------------+-----+\n",
      "|userId|Downgrade|GenderNew|LevelNew|sum_NextSong|sum_thumbsUp|sum_ThumbsDown|label|\n",
      "+------+---------+---------+--------+------------+------------+--------------+-----+\n",
      "|100010|        0|        0|       0|         275|          17|             5|    0|\n",
      "|200002|        0|        1|       1|         387|          21|             6|    1|\n",
      "|   125|        0|        1|       0|           8|           0|             0|    1|\n",
      "|    51|        0|        1|       1|        2111|         100|            21|    1|\n",
      "|   124|        0|        0|       1|        4079|         171|            41|    1|\n",
      "+------+---------+---------+--------+------------+------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "the amount of majority : 171.   the amount of minority : 54 \n",
      "the ratio is 3\n",
      "The amount of records is 333 after oversampling\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|[0.0,1.0,387.0,21...|\n",
      "|    1|       (5,[2],[8.0])|\n",
      "|    1|[0.0,1.0,2111.0,1...|\n",
      "|    1|[0.0,1.0,4079.0,1...|\n",
      "+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_new = creat_features(data_new)\n",
    "#show the new features\n",
    "data_new.select([\"userId\",\"GenderNew\",\"LevelNew\",\"ThumbsUp\", \"ThumbsDown\",\"Downgrade\",\"NextSong\"]).show(5)\n",
    "data_new_features = new_features(data_new)\n",
    "data_new_features.show(5)\n",
    "combined_df = oversampling(data_new_features)\n",
    "combined_df = transfer_feature(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data in train dataset, validation dataset and test dataset.\n",
    "(trainData, tempData)= combined_df.randomSplit(weights = [0.70,0.30])\n",
    "(valiData, testData)= tempData.randomSplit(weights = [0.50,0.50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestClassifier()\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, label_new])\n",
    "param_grid = ParamGridBuilder().\\\n",
    "            addGrid(rf.maxDepth, [4,5,6]).\\\n",
    "            addGrid(rf.numTrees, [5,7]).\\\n",
    "            build()\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps = param_grid, \n",
    "                    evaluator = MulticlassClassificationEvaluator(metricName='f1'),\n",
    "                    numFolds=3)\n",
    "cv_model = cv.fit(trainData)\n",
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: RandomForestClassifier_c1c51f2ff3f0\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "Evaluation on  validation data\n",
      "Accuracy = 0.894737\n",
      "Validation Error = 0.105263\n",
      "F1 score = 0.894737\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "Evaluation on  test data\n",
      "Accuracy = 0.897959\n",
      "Validation Error = 0.102041\n",
      "F1 score = 0.897183\n",
      "-----------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('model:',rf)\n",
    "\n",
    "#run function evaluation on validation dataset\n",
    "predictions = best_model.transform(valiData)\n",
    "evaluation(predictions, 'validation data')\n",
    "\n",
    "#run function evaluation on test dataset\n",
    "predictions = best_model.transform(testData)\n",
    "evaluation(predictions,'test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Conclusion\n",
    "In this project, I worked with the Sparkify dataset and predict whether the user churns or not. \n",
    "I load and clean data at first, Then exploring the data and data engineering. In the modeling session, Training the three models based on three different machine learning algorithms. \n",
    "The final RandomForestClassifier gives me a good evaluation result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
